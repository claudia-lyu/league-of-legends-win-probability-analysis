{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7168648-6fdb-4130-8339-ea55637f227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "League of Legends Win Probability Prediction\n",
    "- Loads Kaggle LoL 10-min dataset\n",
    "- Cleans/engineers features\n",
    "- Trains logistic regression with 5-fold CV\n",
    "- Bootstraps coefficients and accuracy/AUC CIs\n",
    "- Saves artifacts to ./reports\n",
    "\n",
    "Run:\n",
    "  python src/lol_win_model.py --data data/lol_10min.csv --target blueWins\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Config / Defaults\n",
    "# --------------------------\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "FEATURES_DEFAULT = [\n",
    "    \"blueKills\",\n",
    "    \"blueDeaths\",\n",
    "    \"blueGoldDiff\",\n",
    "    \"blueExperienceDiff\",\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "\n",
    "def load_data(path: str, target: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    df = pd.read_csv(path)\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target}' not found in the dataset.\")\n",
    "\n",
    "    # Basic sanity filter (the Kaggle file sometimes includes NaNs)\n",
    "    df = df.dropna(subset=FEATURES_DEFAULT + [target])\n",
    "    X = df[FEATURES_DEFAULT].copy()\n",
    "    y = df[target].astype(int).copy()\n",
    "\n",
    "    # Optional: enforce integer types for kill/death\n",
    "    for c in [\"blueKills\", \"blueDeaths\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].astype(int)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_pipeline() -> Pipeline:\n",
    "    # Liblinear handles small to mid datasets well\n",
    "    logreg = LogisticRegression(\n",
    "        solver=\"liblinear\", random_state=SEED, max_iter=1000\n",
    "    )\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", logreg),\n",
    "    ])\n",
    "\n",
    "\n",
    "def cross_validate(X: pd.DataFrame, y: pd.Series, pipe: Pipeline, n_splits: int = 5) -> Dict[str, float]:\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    acc = cross_val_score(pipe, X, y, scoring=\"accuracy\", cv=kf)\n",
    "    auc = cross_val_score(pipe, X, y, scoring=\"roc_auc\", cv=kf)\n",
    "    return {\n",
    "        \"cv_accuracy_mean\": float(np.mean(acc)),\n",
    "        \"cv_accuracy_std\": float(np.std(acc)),\n",
    "        \"cv_auc_mean\": float(np.mean(auc)),\n",
    "        \"cv_auc_std\": float(np.std(auc)),\n",
    "    }\n",
    "\n",
    "\n",
    "def bootstrap_ci(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    n_boot: int = 1000,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Bootstrap both coefficients (via statsmodels Logit) and performance (acc/auc).\n",
    "    Returns:\n",
    "      coef_df: DataFrame with coefficient bootstrap distribution + CI\n",
    "      perf_ci: dict with accuracy/auc CIs\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    coef_samples = []\n",
    "    acc_samples = []\n",
    "    auc_samples = []\n",
    "\n",
    "    # Prepare a single held-out style split for perf bootstrap (or use full-sample preds)\n",
    "    # We'll refit each bootstrap sample and evaluate on its own OOB complement (approximation).\n",
    "    for b in range(n_boot):\n",
    "        idx = np.random.randint(0, n, size=n)       # sample with replacement\n",
    "        oob = np.setdiff1d(np.arange(n), idx)       # out-of-bag indices\n",
    "\n",
    "        Xb = X.iloc[idx].reset_index(drop=True)\n",
    "        yb = y.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "        # statsmodels for coefficients\n",
    "        Xb_sm = sm.add_constant(Xb)\n",
    "        try:\n",
    "            model = sm.Logit(yb, Xb_sm).fit(disp=False)\n",
    "            coef_samples.append(model.params.values)  # const + features\n",
    "        except Exception:\n",
    "            # if a bootstrap sample fails to converge, skip (rare)\n",
    "            continue\n",
    "\n",
    "        # Performance on OOB if available\n",
    "        if len(oob) > 0:\n",
    "            pipe = make_pipeline()\n",
    "            pipe.fit(Xb, yb)\n",
    "            yhat_proba = pipe.predict_proba(X.iloc[oob])[:, 1]\n",
    "            yhat = (yhat_proba >= 0.5).astype(int)\n",
    "\n",
    "            acc_samples.append(accuracy_score(y.iloc[oob], yhat))\n",
    "            try:\n",
    "                auc_samples.append(roc_auc_score(y.iloc[oob], yhat_proba))\n",
    "            except Exception:\n",
    "                # if OOB has one class, ROC AUC is undefined\n",
    "                pass\n",
    "\n",
    "    coef_samples = np.array(coef_samples)  # shape: [B, k]\n",
    "    columns = [\"const\"] + list(X.columns)\n",
    "    coef_df = pd.DataFrame(coef_samples, columns=columns)\n",
    "\n",
    "    def ci(a, low=5, high=95):\n",
    "        return np.percentile(a, [low, high])\n",
    "\n",
    "    # Build CI table\n",
    "    ci_rows = []\n",
    "    for c in coef_df.columns:\n",
    "        low, high = ci(coef_df[c].values)\n",
    "        ci_rows.append({\"term\": c, \"ci_low\": low, \"ci_high\": high, \"mean\": coef_df[c].mean()})\n",
    "    coef_ci_df = pd.DataFrame(ci_rows)\n",
    "\n",
    "    # Perf CI\n",
    "    perf_ci = {}\n",
    "    if len(acc_samples) > 0:\n",
    "        acc_low, acc_high = ci(np.array(acc_samples))\n",
    "        perf_ci[\"bootstrap_acc_mean\"] = float(np.mean(acc_samples))\n",
    "        perf_ci[\"bootstrap_acc_ci_90\"] = [float(acc_low), float(acc_high)]\n",
    "    if len(auc_samples) > 0:\n",
    "        auc_low, auc_high = ci(np.array(auc_samples))\n",
    "        perf_ci[\"bootstrap_auc_mean\"] = float(np.mean(auc_samples))\n",
    "        perf_ci[\"bootstrap_auc_ci_90\"] = [float(auc_low), float(auc_high)]\n",
    "\n",
    "    return coef_ci_df, perf_ci\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RunOutputs:\n",
    "    metrics: Dict[str, float]\n",
    "    coef_ci: pd.DataFrame\n",
    "\n",
    "\n",
    "def run(data_path: str, target: str) -> RunOutputs:\n",
    "    X, y = load_data(data_path, target)\n",
    "\n",
    "    # CV metrics\n",
    "    pipe = make_pipeline()\n",
    "    metrics = cross_validate(X, y, pipe, n_splits=5)\n",
    "\n",
    "    # Fit once on full data for quick sanity\n",
    "    pipe.fit(X, y)\n",
    "    yhat_proba = pipe.predict_proba(X)[:, 1]\n",
    "    yhat = (yhat_proba >= 0.5).astype(int)\n",
    "    metrics[\"train_accuracy\"] = float(accuracy_score(y, yhat))\n",
    "    try:\n",
    "        metrics[\"train_auc\"] = float(roc_auc_score(y, yhat_proba))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Bootstrap CIs for coefficients + performance\n",
    "    coef_ci_df, perf_ci = bootstrap_ci(X, y, n_boot=1000)\n",
    "    metrics.update(perf_ci)\n",
    "\n",
    "    return RunOutputs(metrics=metrics, coef_ci=coef_ci_df)\n",
    "\n",
    "\n",
    "def save_outputs(outputs: RunOutputs, out_dir: str = \"reports\") -> None:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(outputs.metrics, f, indent=2)\n",
    "    outputs.coef_ci.to_csv(os.path.join(out_dir, \"coef_bootstrap.csv\"), index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, required=True, help=\"Path to Kaggle CSV (10-minute dataset)\")\n",
    "    parser.add_argument(\"--target\", type=str, default=\"blueWins\", help=\"Binary target column (default: blueWins)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    outputs = run(args.data, args.target)\n",
    "    save_outputs(outputs)\n",
    "    print(\"\\n=== Cross-Validation ===\")\n",
    "    for k in [\"cv_accuracy_mean\", \"cv_accuracy_std\", \"cv_auc_mean\", \"cv_auc_std\"]:\n",
    "        if k in outputs.metrics:\n",
    "            print(f\"{k}: {outputs.metrics[k]:.4f}\")\n",
    "    print(\"\\n=== Bootstrap (90% CI) ===\")\n",
    "    if \"bootstrap_acc_ci_90\" in outputs.metrics:\n",
    "        lo, hi = outputs.metrics[\"bootstrap_acc_ci_90\"]\n",
    "        print(f\"Accuracy ~ mean {outputs.metrics['bootstrap_acc_mean']:.4f} | CI90 [{lo:.4f}, {hi:.4f}]\")\n",
    "    if \"bootstrap_auc_ci_90\" in outputs.metrics:\n",
    "        lo, hi = outputs.metrics[\"bootstrap_auc_ci_90\"]\n",
    "        print(f\"AUC ~ mean {outputs.metrics['bootstrap_auc_mean']:.4f} | CI90 [{lo:.4f}, {hi:.4f}]\")\n",
    "    print(\"\\nSaved: reports/metrics.json & reports/coef_bootstrap.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dso576",
   "language": "python",
   "name": "dso576"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
